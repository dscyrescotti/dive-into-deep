{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28)\n[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n    0   1   4   0   0   0   0   1   1   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n   54   0   0   0   1   3   4   0   0   3]\n [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n  144 123  23   0   0   0   0  12  10   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n  107 156 161 109  64  23  77 130  72  15]\n [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n  216 163 127 121 122 146 141  88 172  66]\n [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n  223 223 215 213 164 127 123 196 229   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n  235 227 224 222 224 221 223 245 173   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n  180 212 210 211 213 223 220 243 202   0]\n [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n  169 227 208 218 224 212 226 197 209  52]\n [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n  198 221 215 213 222 220 245 119 167  56]\n [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n  232 213 218 223 234 217 217 209  92   0]\n [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n  222 221 216 223 229 215 218 255  77   0]\n [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n  211 218 224 223 219 215 224 244 159   0]\n [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n  224 234 176 188 250 248 233 238 215   0]\n [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n  255 255 221 234 221 211 220 232 246   0]\n [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n  188 154 191 210 204 209 222 228 225   0]\n [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n  168 219 221 215 217 223 223 224 229  29]\n [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n  239 223 218 212 209 222 220 221 230  67]\n [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n  199 206 186 181 177 172 181 205 206 115]\n [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n  195 191 198 192 176 156 167 177 210  92]\n [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n  210 210 211 188 188 194 192 216 170   0]\n [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n  182 182 181 176 166 168  99  58   0   0]\n [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]]\n(60000,)\n9\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_images[0])\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [ \"T-shirt/top\" , 'Trouser' , 'Pullover' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Ankle boot' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.        ]\n  [0.        ]\n  [0.05098039]\n  [0.28627452]\n  [0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.01568628]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.00392157]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.01176471]\n  [0.        ]\n  [0.14117648]\n  [0.53333336]\n  [0.49803922]\n  [0.24313726]\n  [0.21176471]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.01176471]\n  [0.01568628]\n  [0.        ]\n  [0.        ]\n  [0.01176471]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.02352941]\n  [0.        ]\n  [0.4       ]\n  [0.8       ]\n  [0.6901961 ]\n  [0.5254902 ]\n  [0.5647059 ]\n  [0.48235294]\n  [0.09019608]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.04705882]\n  [0.03921569]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.60784316]\n  [0.9254902 ]\n  [0.8117647 ]\n  [0.69803923]\n  [0.41960785]\n  [0.6117647 ]\n  [0.6313726 ]\n  [0.42745098]\n  [0.2509804 ]\n  [0.09019608]\n  [0.3019608 ]\n  [0.50980395]\n  [0.28235295]\n  [0.05882353]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.        ]\n  [0.27058825]\n  [0.8117647 ]\n  [0.8745098 ]\n  [0.85490197]\n  [0.84705883]\n  [0.84705883]\n  [0.6392157 ]\n  [0.49803922]\n  [0.4745098 ]\n  [0.47843137]\n  [0.57254905]\n  [0.5529412 ]\n  [0.34509805]\n  [0.6745098 ]\n  [0.25882354]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.00392157]\n  [0.00392157]\n  [0.        ]\n  [0.78431374]\n  [0.9098039 ]\n  [0.9098039 ]\n  [0.9137255 ]\n  [0.8980392 ]\n  [0.8745098 ]\n  [0.8745098 ]\n  [0.84313726]\n  [0.8352941 ]\n  [0.6431373 ]\n  [0.49803922]\n  [0.48235294]\n  [0.76862746]\n  [0.8980392 ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.7176471 ]\n  [0.88235295]\n  [0.84705883]\n  [0.8745098 ]\n  [0.89411765]\n  [0.92156863]\n  [0.8901961 ]\n  [0.8784314 ]\n  [0.87058824]\n  [0.8784314 ]\n  [0.8666667 ]\n  [0.8745098 ]\n  [0.9607843 ]\n  [0.6784314 ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.75686276]\n  [0.89411765]\n  [0.85490197]\n  [0.8352941 ]\n  [0.7764706 ]\n  [0.7058824 ]\n  [0.83137256]\n  [0.8235294 ]\n  [0.827451  ]\n  [0.8352941 ]\n  [0.8745098 ]\n  [0.8627451 ]\n  [0.9529412 ]\n  [0.7921569 ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.01176471]\n  [0.        ]\n  [0.04705882]\n  [0.85882354]\n  [0.8627451 ]\n  [0.83137256]\n  [0.85490197]\n  [0.7529412 ]\n  [0.6627451 ]\n  [0.8901961 ]\n  [0.8156863 ]\n  [0.85490197]\n  [0.8784314 ]\n  [0.83137256]\n  [0.8862745 ]\n  [0.77254903]\n  [0.81960785]\n  [0.20392157]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.02352941]\n  [0.        ]\n  [0.3882353 ]\n  [0.95686275]\n  [0.87058824]\n  [0.8627451 ]\n  [0.85490197]\n  [0.79607844]\n  [0.7764706 ]\n  [0.8666667 ]\n  [0.84313726]\n  [0.8352941 ]\n  [0.87058824]\n  [0.8627451 ]\n  [0.9607843 ]\n  [0.46666667]\n  [0.654902  ]\n  [0.21960784]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.01568628]\n  [0.        ]\n  [0.        ]\n  [0.21568628]\n  [0.9254902 ]\n  [0.89411765]\n  [0.9019608 ]\n  [0.89411765]\n  [0.9411765 ]\n  [0.9098039 ]\n  [0.8352941 ]\n  [0.85490197]\n  [0.8745098 ]\n  [0.91764706]\n  [0.8509804 ]\n  [0.8509804 ]\n  [0.81960785]\n  [0.36078432]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.00392157]\n  [0.01568628]\n  [0.02352941]\n  [0.02745098]\n  [0.00784314]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.92941177]\n  [0.8862745 ]\n  [0.8509804 ]\n  [0.8745098 ]\n  [0.87058824]\n  [0.85882354]\n  [0.87058824]\n  [0.8666667 ]\n  [0.84705883]\n  [0.8745098 ]\n  [0.8980392 ]\n  [0.84313726]\n  [0.85490197]\n  [1.        ]\n  [0.3019608 ]\n  [0.        ]]\n\n [[0.        ]\n  [0.01176471]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.24313726]\n  [0.5686275 ]\n  [0.8       ]\n  [0.89411765]\n  [0.8117647 ]\n  [0.8352941 ]\n  [0.8666667 ]\n  [0.85490197]\n  [0.8156863 ]\n  [0.827451  ]\n  [0.85490197]\n  [0.8784314 ]\n  [0.8745098 ]\n  [0.85882354]\n  [0.84313726]\n  [0.8784314 ]\n  [0.95686275]\n  [0.62352943]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.07058824]\n  [0.17254902]\n  [0.32156864]\n  [0.41960785]\n  [0.7411765 ]\n  [0.89411765]\n  [0.8627451 ]\n  [0.87058824]\n  [0.8509804 ]\n  [0.8862745 ]\n  [0.78431374]\n  [0.8039216 ]\n  [0.827451  ]\n  [0.9019608 ]\n  [0.8784314 ]\n  [0.91764706]\n  [0.6901961 ]\n  [0.7372549 ]\n  [0.98039216]\n  [0.972549  ]\n  [0.9137255 ]\n  [0.93333334]\n  [0.84313726]\n  [0.        ]]\n\n [[0.        ]\n  [0.22352941]\n  [0.73333335]\n  [0.8156863 ]\n  [0.8784314 ]\n  [0.8666667 ]\n  [0.8784314 ]\n  [0.8156863 ]\n  [0.8       ]\n  [0.8392157 ]\n  [0.8156863 ]\n  [0.81960785]\n  [0.78431374]\n  [0.62352943]\n  [0.9607843 ]\n  [0.75686276]\n  [0.80784315]\n  [0.8745098 ]\n  [1.        ]\n  [1.        ]\n  [0.8666667 ]\n  [0.91764706]\n  [0.8666667 ]\n  [0.827451  ]\n  [0.8627451 ]\n  [0.9098039 ]\n  [0.9647059 ]\n  [0.        ]]\n\n [[0.01176471]\n  [0.7921569 ]\n  [0.89411765]\n  [0.8784314 ]\n  [0.8666667 ]\n  [0.827451  ]\n  [0.827451  ]\n  [0.8392157 ]\n  [0.8039216 ]\n  [0.8039216 ]\n  [0.8039216 ]\n  [0.8627451 ]\n  [0.9411765 ]\n  [0.3137255 ]\n  [0.5882353 ]\n  [1.        ]\n  [0.8980392 ]\n  [0.8666667 ]\n  [0.7372549 ]\n  [0.6039216 ]\n  [0.7490196 ]\n  [0.8235294 ]\n  [0.8       ]\n  [0.81960785]\n  [0.87058824]\n  [0.89411765]\n  [0.88235295]\n  [0.        ]]\n\n [[0.38431373]\n  [0.9137255 ]\n  [0.7764706 ]\n  [0.8235294 ]\n  [0.87058824]\n  [0.8980392 ]\n  [0.8980392 ]\n  [0.91764706]\n  [0.9764706 ]\n  [0.8627451 ]\n  [0.7607843 ]\n  [0.84313726]\n  [0.8509804 ]\n  [0.94509804]\n  [0.25490198]\n  [0.28627452]\n  [0.41568628]\n  [0.45882353]\n  [0.65882355]\n  [0.85882354]\n  [0.8666667 ]\n  [0.84313726]\n  [0.8509804 ]\n  [0.8745098 ]\n  [0.8745098 ]\n  [0.8784314 ]\n  [0.8980392 ]\n  [0.11372549]]\n\n [[0.29411766]\n  [0.8       ]\n  [0.83137256]\n  [0.8       ]\n  [0.75686276]\n  [0.8039216 ]\n  [0.827451  ]\n  [0.88235295]\n  [0.84705883]\n  [0.7254902 ]\n  [0.77254903]\n  [0.80784315]\n  [0.7764706 ]\n  [0.8352941 ]\n  [0.9411765 ]\n  [0.7647059 ]\n  [0.8901961 ]\n  [0.9607843 ]\n  [0.9372549 ]\n  [0.8745098 ]\n  [0.85490197]\n  [0.83137256]\n  [0.81960785]\n  [0.87058824]\n  [0.8627451 ]\n  [0.8666667 ]\n  [0.9019608 ]\n  [0.2627451 ]]\n\n [[0.1882353 ]\n  [0.79607844]\n  [0.7176471 ]\n  [0.7607843 ]\n  [0.8352941 ]\n  [0.77254903]\n  [0.7254902 ]\n  [0.74509805]\n  [0.7607843 ]\n  [0.7529412 ]\n  [0.7921569 ]\n  [0.8392157 ]\n  [0.85882354]\n  [0.8666667 ]\n  [0.8627451 ]\n  [0.9254902 ]\n  [0.88235295]\n  [0.84705883]\n  [0.78039217]\n  [0.80784315]\n  [0.7294118 ]\n  [0.70980394]\n  [0.69411767]\n  [0.6745098 ]\n  [0.70980394]\n  [0.8039216 ]\n  [0.80784315]\n  [0.4509804 ]]\n\n [[0.        ]\n  [0.47843137]\n  [0.85882354]\n  [0.75686276]\n  [0.7019608 ]\n  [0.67058825]\n  [0.7176471 ]\n  [0.76862746]\n  [0.8       ]\n  [0.8235294 ]\n  [0.8352941 ]\n  [0.8117647 ]\n  [0.827451  ]\n  [0.8235294 ]\n  [0.78431374]\n  [0.76862746]\n  [0.7607843 ]\n  [0.7490196 ]\n  [0.7647059 ]\n  [0.7490196 ]\n  [0.7764706 ]\n  [0.7529412 ]\n  [0.6901961 ]\n  [0.6117647 ]\n  [0.654902  ]\n  [0.69411767]\n  [0.8235294 ]\n  [0.36078432]]\n\n [[0.        ]\n  [0.        ]\n  [0.2901961 ]\n  [0.7411765 ]\n  [0.83137256]\n  [0.7490196 ]\n  [0.6862745 ]\n  [0.6745098 ]\n  [0.6862745 ]\n  [0.70980394]\n  [0.7254902 ]\n  [0.7372549 ]\n  [0.7411765 ]\n  [0.7372549 ]\n  [0.75686276]\n  [0.7764706 ]\n  [0.8       ]\n  [0.81960785]\n  [0.8235294 ]\n  [0.8235294 ]\n  [0.827451  ]\n  [0.7372549 ]\n  [0.7372549 ]\n  [0.7607843 ]\n  [0.7529412 ]\n  [0.84705883]\n  [0.6666667 ]\n  [0.        ]]\n\n [[0.00784314]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.25882354]\n  [0.78431374]\n  [0.87058824]\n  [0.92941177]\n  [0.9372549 ]\n  [0.9490196 ]\n  [0.9647059 ]\n  [0.9529412 ]\n  [0.95686275]\n  [0.8666667 ]\n  [0.8627451 ]\n  [0.75686276]\n  [0.7490196 ]\n  [0.7019608 ]\n  [0.7137255 ]\n  [0.7137255 ]\n  [0.70980394]\n  [0.6901961 ]\n  [0.6509804 ]\n  [0.65882355]\n  [0.3882353 ]\n  [0.22745098]\n  [0.        ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.15686275]\n  [0.23921569]\n  [0.17254902]\n  [0.28235295]\n  [0.16078432]\n  [0.13725491]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]]\n\n [[0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]\n  [0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1600)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               204928    \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 241,546\nTrainable params: 241,546\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000, 28, 28, 1)\n(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x = train_images[:50000], train_images[50000:]\n",
    "train_y, val_y = train_labels[:50000], train_labels[50000:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "118/118 [==============================] - 30s 246ms/step - loss: 1.1701 - acc: 0.5792\n",
      "Epoch 2/20\n",
      "118/118 [==============================] - 32s 271ms/step - loss: 0.5265 - acc: 0.8033\n",
      "Epoch 3/20\n",
      "118/118 [==============================] - 33s 283ms/step - loss: 0.4310 - acc: 0.8415\n",
      "Epoch 4/20\n",
      "118/118 [==============================] - 34s 287ms/step - loss: 0.3775 - acc: 0.8604\n",
      "Epoch 5/20\n",
      "118/118 [==============================] - 34s 290ms/step - loss: 0.3371 - acc: 0.8742\n",
      "Epoch 6/20\n",
      "118/118 [==============================] - 35s 294ms/step - loss: 0.3033 - acc: 0.8867\n",
      "Epoch 7/20\n",
      "118/118 [==============================] - 42s 356ms/step - loss: 0.2886 - acc: 0.8923\n",
      "Epoch 8/20\n",
      "118/118 [==============================] - 44s 375ms/step - loss: 0.2678 - acc: 0.8993\n",
      "Epoch 9/20\n",
      "118/118 [==============================] - 40s 337ms/step - loss: 0.2500 - acc: 0.9066\n",
      "Epoch 10/20\n",
      "118/118 [==============================] - 38s 325ms/step - loss: 0.2333 - acc: 0.9124\n",
      "Epoch 11/20\n",
      "118/118 [==============================] - 39s 329ms/step - loss: 0.2249 - acc: 0.9158\n",
      "Epoch 12/20\n",
      "118/118 [==============================] - 41s 345ms/step - loss: 0.2099 - acc: 0.9202\n",
      "Epoch 13/20\n",
      "118/118 [==============================] - 39s 330ms/step - loss: 0.2001 - acc: 0.9244\n",
      "Epoch 14/20\n",
      "118/118 [==============================] - 39s 333ms/step - loss: 0.1835 - acc: 0.9319\n",
      "Epoch 15/20\n",
      "118/118 [==============================] - 42s 356ms/step - loss: 0.1750 - acc: 0.9338\n",
      "Epoch 16/20\n",
      "118/118 [==============================] - 40s 342ms/step - loss: 0.1624 - acc: 0.9392\n",
      "Epoch 17/20\n",
      "118/118 [==============================] - 40s 338ms/step - loss: 0.1593 - acc: 0.9411\n",
      "Epoch 18/20\n",
      "118/118 [==============================] - 41s 345ms/step - loss: 0.1456 - acc: 0.9458\n",
      "Epoch 19/20\n",
      "118/118 [==============================] - 39s 333ms/step - loss: 0.1326 - acc: 0.9506\n",
      "Epoch 20/20\n",
      "118/118 [==============================] - 39s 333ms/step - loss: 0.1226 - acc: 0.9548\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd7849361f0>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size=512, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3043 - acc: 0.9051\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3042638599872589 0.9050999879837036\n"
     ]
    }
   ],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not bad result"
   ]
  }
 ]
}